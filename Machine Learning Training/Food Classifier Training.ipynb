{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Load dataset\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport torch\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\n\n'''\nfor dirname, _, filenames in os.walk('/kaggle/input/fruits/fruits-360_dataset/fruits-360/Training'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''\n\n# Define data transformations (e.g., flipping and normalization)\ntransform = transforms.Compose([\n    transforms.ToTensor(), # Convert images to PyTorch tensors\n    transforms.RandomHorizontalFlip(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize pixel values to [-1, 1]\n])\n\n# Create train and test set from directories\n'''\ntrain_set = datasets.ImageFolder(root='/kaggle/input/fruits/fruits-360_dataset/fruits-360/Training', transform=transform)\ntest_set = datasets.ImageFolder(root='/kaggle/input/fruits/fruits-360_dataset/fruits-360/Test', transform=transform)\n'''\ndataset = ImageFolder(root='/kaggle/input/reduced-dataset/dataset', transform=transform)\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\n# Create data loaders for training and test sets\nbatch_size = 32  # You can adjust this according to your system's memory\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-19T03:40:09.733727Z","iopub.execute_input":"2023-10-19T03:40:09.733971Z","iopub.status.idle":"2023-10-19T03:40:20.567452Z","shell.execute_reply.started":"2023-10-19T03:40:09.733949Z","shell.execute_reply":"2023-10-19T03:40:20.566758Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models\n\n# Model - used pre-trained ones from pytorch\nmodel = models.resnet50()\n\n'''\n# Freeze all layers except the final classification layer\nfor param in model.parameters():\n    param.requires_grad = False\n'''\n\nnum_classes = 6\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)  # Replace the final fully connected layer\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.9)","metadata":{"execution":{"iopub.status.busy":"2023-10-19T03:40:20.568865Z","iopub.execute_input":"2023-10-19T03:40:20.569229Z","iopub.status.idle":"2023-10-19T03:40:21.013458Z","shell.execute_reply.started":"2023-10-19T03:40:20.569206Z","shell.execute_reply":"2023-10-19T03:40:21.012770Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Train\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nnum_epochs = 100  # Adjust as needed\nfor epoch in range(num_epochs):\n    model.train()\n    totalTrainLoss = 0.0\n    for i, (inputs, labels) in enumerate(train_loader):\n        (inputs, labels) = (inputs.to(device), labels.to(torch.long).to(device))\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        totalTrainLoss += loss\n    print(\"Epoch\", epoch, \"Training Loss:\", totalTrainLoss.item())","metadata":{"execution":{"iopub.status.busy":"2023-10-19T03:40:21.014435Z","iopub.execute_input":"2023-10-19T03:40:21.014753Z","iopub.status.idle":"2023-10-19T04:24:56.628004Z","shell.execute_reply.started":"2023-10-19T03:40:21.014724Z","shell.execute_reply":"2023-10-19T04:24:56.627150Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Epoch 0 Training Loss: 1790.3992919921875\nEpoch 1 Training Loss: 983.5216064453125\nEpoch 2 Training Loss: 717.9075927734375\nEpoch 3 Training Loss: 584.914306640625\nEpoch 4 Training Loss: 562.4871826171875\nEpoch 5 Training Loss: 553.5817260742188\nEpoch 6 Training Loss: 472.6867370605469\nEpoch 7 Training Loss: 560.9715576171875\nEpoch 8 Training Loss: 488.4855041503906\nEpoch 9 Training Loss: 501.5417175292969\nEpoch 10 Training Loss: 534.8219604492188\nEpoch 11 Training Loss: 510.9921569824219\nEpoch 12 Training Loss: 521.037353515625\nEpoch 13 Training Loss: 502.8099060058594\nEpoch 14 Training Loss: 482.532958984375\nEpoch 15 Training Loss: 452.2392883300781\nEpoch 16 Training Loss: 414.15863037109375\nEpoch 17 Training Loss: 518.8253173828125\nEpoch 18 Training Loss: 489.9633483886719\nEpoch 19 Training Loss: 565.2818603515625\nEpoch 20 Training Loss: 471.59796142578125\nEpoch 21 Training Loss: 433.0090637207031\nEpoch 22 Training Loss: 404.0568542480469\nEpoch 23 Training Loss: 431.2936096191406\nEpoch 24 Training Loss: 432.310791015625\nEpoch 25 Training Loss: 414.79107666015625\nEpoch 26 Training Loss: 359.1354675292969\nEpoch 27 Training Loss: 462.3246765136719\nEpoch 28 Training Loss: 457.4349365234375\nEpoch 29 Training Loss: 465.9557189941406\nEpoch 30 Training Loss: 419.9644470214844\nEpoch 31 Training Loss: 330.9003601074219\nEpoch 32 Training Loss: 360.8304138183594\nEpoch 33 Training Loss: 358.7156066894531\nEpoch 34 Training Loss: 384.2756652832031\nEpoch 35 Training Loss: 397.227783203125\nEpoch 36 Training Loss: 477.1842346191406\nEpoch 37 Training Loss: 418.1459045410156\nEpoch 38 Training Loss: 431.32769775390625\nEpoch 39 Training Loss: 336.38116455078125\nEpoch 40 Training Loss: 471.1042175292969\nEpoch 41 Training Loss: 475.8218688964844\nEpoch 42 Training Loss: 352.9716796875\nEpoch 43 Training Loss: 400.8583984375\nEpoch 44 Training Loss: 516.816650390625\nEpoch 45 Training Loss: 334.6751403808594\nEpoch 46 Training Loss: 424.1917724609375\nEpoch 47 Training Loss: 380.3211669921875\nEpoch 48 Training Loss: 440.8957214355469\nEpoch 49 Training Loss: 372.3518981933594\nEpoch 50 Training Loss: 379.2710266113281\nEpoch 51 Training Loss: 335.13946533203125\nEpoch 52 Training Loss: 486.64227294921875\nEpoch 53 Training Loss: 447.1807861328125\nEpoch 54 Training Loss: 383.6684265136719\nEpoch 55 Training Loss: 402.6828918457031\nEpoch 56 Training Loss: 345.1987609863281\nEpoch 57 Training Loss: 467.3868713378906\nEpoch 58 Training Loss: 387.4161682128906\nEpoch 59 Training Loss: 403.6870422363281\nEpoch 60 Training Loss: 361.6359558105469\nEpoch 61 Training Loss: 406.1297607421875\nEpoch 62 Training Loss: 380.21392822265625\nEpoch 63 Training Loss: 408.8575744628906\nEpoch 64 Training Loss: 398.85498046875\nEpoch 65 Training Loss: 359.6394348144531\nEpoch 66 Training Loss: 407.4143981933594\nEpoch 67 Training Loss: 346.9753112792969\nEpoch 68 Training Loss: 400.4359436035156\nEpoch 69 Training Loss: 359.07135009765625\nEpoch 70 Training Loss: 347.7909851074219\nEpoch 71 Training Loss: 425.86456298828125\nEpoch 72 Training Loss: 434.4263916015625\nEpoch 73 Training Loss: 399.263671875\nEpoch 74 Training Loss: 361.4377746582031\nEpoch 75 Training Loss: 394.4692687988281\nEpoch 76 Training Loss: 380.67291259765625\nEpoch 77 Training Loss: 370.08856201171875\nEpoch 78 Training Loss: 348.1556701660156\nEpoch 79 Training Loss: 403.5406799316406\nEpoch 80 Training Loss: 386.894775390625\nEpoch 81 Training Loss: 340.5224609375\nEpoch 82 Training Loss: 384.76702880859375\nEpoch 83 Training Loss: 319.8314514160156\nEpoch 84 Training Loss: 476.9050598144531\nEpoch 85 Training Loss: 421.9031982421875\nEpoch 86 Training Loss: 397.0712890625\nEpoch 87 Training Loss: 389.3070068359375\nEpoch 88 Training Loss: 492.0409851074219\nEpoch 89 Training Loss: 434.6925048828125\nEpoch 90 Training Loss: 400.0413513183594\nEpoch 91 Training Loss: 396.32177734375\nEpoch 92 Training Loss: 375.4556579589844\nEpoch 93 Training Loss: 480.29449462890625\nEpoch 94 Training Loss: 466.83984375\nEpoch 95 Training Loss: 384.50518798828125\nEpoch 96 Training Loss: 351.9159240722656\nEpoch 97 Training Loss: 369.7982177734375\nEpoch 98 Training Loss: 364.4720153808594\nEpoch 99 Training Loss: 388.429931640625\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test\nmodel.eval()\ncorrects = 0\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n        corrects += torch.sum(preds == labels.data)\n\ntest_acc = corrects.double() / len(test_dataset)\nprint(f'Test Accuracy: {test_acc:.4f}')\n\n# Specify the file path where you want to save the model\nsave_path = '/kaggle/working/model.pth'\n\n# Save the model (including architecture and weights)\ntorch.save(model, save_path)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-19T08:23:04.799295Z","iopub.execute_input":"2023-10-19T08:23:04.800460Z","iopub.status.idle":"2023-10-19T08:23:19.873699Z","shell.execute_reply.started":"2023-10-19T08:23:04.800418Z","shell.execute_reply":"2023-10-19T08:23:19.872564Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Test Accuracy: 0.8021\n","output_type":"stream"}]},{"cell_type":"code","source":"#second test\nfrom PIL import Image\n\n# map class names to class number\nroot_directory = '/kaggle/input/reduced-dataset/dataset'\nclass_directories = sorted([d for d in os.listdir(root_directory) if os.path.isdir(os.path.join(root_directory, d))])\nclass_mapping = {}\nfor i, class_dir in enumerate(class_directories):\n    class_mapping[i] = class_dir\n    \n#define GPU usage\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the entire model\nmodel = torch.load(save_path)\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Load and preprocess the new image\ntransform = transforms.Compose([\n    transforms.Resize((100, 100)),  # Resize as per your model's input size\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize pixel values to [-1, 1]\n])\n\nimage_path = '/kaggle/input/irl-test/bunch of banana.jpg'\nimage = Image.open(image_path)\nimage = transform(image).unsqueeze(0)  # Add a batch dimension\nimage = image.to(device) # convert data to be GPU suitable\n\n# Perform inference\nwith torch.no_grad():\n    outputs = model(image)\n\n# Convert probabilities to class scores\nclass_scores = torch.softmax(outputs, dim=1)\n\n# Get the predicted class\n_, predicted_class = torch.max(class_scores, 1)\n\n# Print or use the predicted class and class scores\nprint(f'Predicted Class: {predicted_class.item()}')\nprint(f'Predicted Class: {class_mapping[predicted_class.item()]}')","metadata":{"execution":{"iopub.status.busy":"2023-10-19T08:24:47.874868Z","iopub.execute_input":"2023-10-19T08:24:47.875691Z","iopub.status.idle":"2023-10-19T08:24:48.210085Z","shell.execute_reply.started":"2023-10-19T08:24:47.875664Z","shell.execute_reply":"2023-10-19T08:24:48.209217Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Predicted Class: 1\nPredicted Class: Banana\n","output_type":"stream"}]}]}